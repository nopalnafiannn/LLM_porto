{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455fcf2b-07ec-43d0-a2b2-721cc0b5fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb77843-1c09-430c-8873-2b63ea4c3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790661c5-7b59-45c1-9a63-299616755091",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23bcac94-e46e-4ac0-9a26-71d322e78686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 123\n",
      "Document types found: {'products', 'contracts', 'employees', 'company'}\n"
     ]
    }
   ],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba96a03-bd71-412b-a884-60bbdef0fd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb48c4aa-de49-44c7-a894-316fada541a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (2.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: google-auth-httplib2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: google-api-python-client in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (2.159.0)\n",
      "Requirement already satisfied: imapclient in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth-oauthlib) (2.0.0)\n",
      "Requirement already satisfied: httplib2>=0.19.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth-httplib2) (0.22.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-python-client) (2.24.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (5.28.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client imapclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fdd2f75-c2a3-47eb-ae70-673324380c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vector_db', 'Cover Letter Generator.ipynb', 'knowledge-base', 'credentials.json', '.gradio', '.ipynb_checkpoints', 'rag_implementation.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List files in the current directory\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c27b0f-8313-4838-8d20-acaf67383044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=75585877953-gbc23lsmbmesd8l1idufa3klrnmoid35.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A62382%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=GBcj31gFxdoR0nzjZIMHMWUCUtuVdC&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/gmail.readonly\"]\n",
    "\n",
    "flow = InstalledAppFlow.from_client_secrets_file(\"credentials.json\", SCOPES)\n",
    "creds = flow.run_local_server(port=0)  # Opens a browser to authenticate\n",
    "\n",
    "# Save credentials for reuse\n",
    "import pickle\n",
    "with open(\"token.pickle\", \"wb\") as token_file:\n",
    "    pickle.dump(creds, token_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa6b1079-46b3-436a-baa2-e4b9ce001e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Email ID: 194ec572b8f987c2\n",
      "   üè∑Ô∏è Subject: Your Upcoming Interview with Education Pioneers\n",
      "   üì§ From: Education Pioneers sent by AppointmentPlus <noreply@appointment-plus.com>\n",
      "   üîç Snippet: Greetings Naufal, Just a quick note to remind you that you have an upcoming interview with a member of the Education Pioneers team. Please note that the interview time below is listed in EASTERN TIME.\n",
      "\n",
      "üìß Email ID: 194ebd5b4d7172a8\n",
      "   üè∑Ô∏è Subject: Spreadsheet shared with you: \"10 Feb - CMU Marketing Interview\"\n",
      "   üì§ From: \"Mohamad Naufal Nafian (via Google Sheets)\" <drive-shares-dm-noreply@google.com>\n",
      "   üîç Snippet: Mohamad Naufal Nafian shared a spreadsheet Mohamad Naufal Nafian (mnafian@andrew.cmu.edu) has invited you to edit the following spreadsheet: 10 Feb - CMU Marketing Interview Open Google LLC, 1600\n",
      "\n",
      "üìß Email ID: 194ebac152b12cbc\n",
      "   üè∑Ô∏è Subject: Re: All Questionnaires Received\n",
      "   üì§ From: Vosyn Recruitment <no-reply@vosyn.breezy-mail.com>\n",
      "   üîç Snippet: Dear Naufal, Thank you for submitting your all responses to our questionnaire. We appreciate your time to complete this. We will revert back to you as soon as we can with any further steps after\n",
      "\n",
      "üìß Email ID: 194eb62241b8a29d\n",
      "   üè∑Ô∏è Subject: Your saved job at Archived 12/20/2024 is about to close\n",
      "   üì§ From: Handshake <handshake@notifications.joinhandshake.com>\n",
      "   üîç Snippet: Handshake Last chance to apply Applications for Business Analyst Intern are due Wednesday, February 12 at 9 AM EST. You got this! Archived 12/20/2024 Other Industries Business Analyst Intern $50‚Äì60/hr\n",
      "\n",
      "üìß Email ID: 194eb61f8d7ed896\n",
      "   üè∑Ô∏è Subject: Your saved job at Archived 12/20/2024 is about to close\n",
      "   üì§ From: Handshake <handshake@notifications.joinhandshake.com>\n",
      "   üîç Snippet: Handshake Last chance to apply Applications for Data Analyst Intern are due Wednesday, February 12 at 9 AM EST. You got this! Archived 12/20/2024 Other Industries Data Analyst Intern $50‚Äì60/hr Remote\n",
      "\n",
      "üìß Email ID: 194eb55dcc47457b\n",
      "   üè∑Ô∏è Subject: Accepted: Discussion External Team (Babak 2) @ Sun Feb 9, 2025 7:30pm - 8:30pm (EST) (Rafael George)\n",
      "   üì§ From: Mohamad Naufal Nafian <mnafian@andrew.cmu.edu>\n",
      "   üîç Snippet: Discussion External Team (Babak 2) Mohamad Naufal Nafian has accepted this invitation. Join with Google Meet Meeting link meet.google.com/eqi-dohy-deh Join by phone (US) +1 505-979-5355 PIN: 989462740\n",
      "\n",
      "üìß Email ID: 194eb07460c59a4b\n",
      "   üè∑Ô∏è Subject: Mohamad Naufal, Register for the STEAM Career Fair & Upcoming Employer Events\n",
      "   üì§ From: \"Career & Professional Development Center\" <handshake@mail.joinhandshake.com>\n",
      "   üîç Snippet: Good morning, Mohamad Naufal Below are a few reminders about upcoming CPDC career fairs, workshops, employer events, and other career opportunities. Upcoming Career Fairs üé®üî¨ STEAM Career Fair\n",
      "\n",
      "üìß Email ID: 194eacb1e6e7e31a\n",
      "   üè∑Ô∏è Subject: Your saved job at Admiral Insurance is about to close\n",
      "   üì§ From: Handshake <handshake@notifications.joinhandshake.com>\n",
      "   üîç Snippet: Handshake Last chance to apply Applications for Summer Intern, Underwriting are due Wednesday, February 12 at 6 AM EST. You got this! Admiral Insurance Insurance Summer Intern, Underwriting $20‚Äì30/hr\n",
      "\n",
      "üìß Email ID: 194eac824480788f\n",
      "   üè∑Ô∏è Subject: Your saved job at Guidewell is about to close\n",
      "   üì§ From: Handshake <handshake@notifications.joinhandshake.com>\n",
      "   üîç Snippet: Handshake Last chance to apply Applications for Analytics Intern are due Wednesday, February 12 at 6 AM EST. You got this! Guidewell Healthcare Analytics Intern $22‚Äì25/hr Onsite Internship Jacksonville\n",
      "\n",
      "üìß Email ID: 194eac801da5685c\n",
      "   üè∑Ô∏è Subject: Your saved job at Guidewell is about to close\n",
      "   üì§ From: Handshake <handshake@notifications.joinhandshake.com>\n",
      "   üîç Snippet: Handshake Last chance to apply Applications for Analytics Intern are due Wednesday, February 12 at 6 AM EST. You got this! Guidewell Healthcare Analytics Intern $22‚Äì25/hr Onsite Internship Jacksonville\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pickle\n",
    "\n",
    "# Load saved credentials\n",
    "with open(\"token.pickle\", \"rb\") as token_file:\n",
    "    creds = pickle.load(token_file)\n",
    "\n",
    "service = build(\"gmail\", \"v1\", credentials=creds)\n",
    "\n",
    "# Fetch latest 10 emails\n",
    "results = service.users().messages().list(userId=\"me\", maxResults=10).execute()\n",
    "messages = results.get(\"messages\", [])\n",
    "\n",
    "for msg in messages:\n",
    "    msg_id = msg[\"id\"]\n",
    "    message = service.users().messages().get(userId=\"me\", id=msg_id, format=\"full\").execute()\n",
    "    \n",
    "    # Extract headers\n",
    "    headers = message[\"payload\"][\"headers\"]\n",
    "    \n",
    "    # Extract useful email information\n",
    "    subject = next((h[\"value\"] for h in headers if h[\"name\"] == \"Subject\"), \"No Subject\")\n",
    "    sender = next((h[\"value\"] for h in headers if h[\"name\"] == \"From\"), \"Unknown Sender\")\n",
    "    snippet = message.get(\"snippet\", \"No Snippet Available\")\n",
    "    \n",
    "    # Print Email Details\n",
    "    print(f\"üìß Email ID: {msg_id}\")\n",
    "    print(f\"   üè∑Ô∏è Subject: {subject}\")\n",
    "    print(f\"   üì§ From: {sender}\")\n",
    "    print(f\"   üîç Snippet: {snippet}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "19dc4678-e5fa-41d5-907c-53d25799b8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 4504\n",
      "Email categories found: {'recruitment', 'general', 'school', 'promotions', 'other'}\n",
      "üì© First processed email timestamp: 0\n",
      "üì© Last processed email timestamp: 1739134877000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "from googleapiclient.discovery import build\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# ‚úÖ Load a pre-trained NLP model for embedding (for better retrieval)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# üîπ Load Gmail API Credentials\n",
    "def load_gmail_service():\n",
    "    with open(\"token.pickle\", \"rb\") as token_file:\n",
    "        creds = pickle.load(token_file)\n",
    "    service = build(\"gmail\", \"v1\", credentials=creds)\n",
    "    return service\n",
    "\n",
    "# üîπ Fetch Latest Emails with Timestamp\n",
    "def fetch_emails(service, max_results=500):\n",
    "    results = service.users().messages().list(userId=\"me\", maxResults=max_results).execute()\n",
    "    messages = results.get(\"messages\", [])\n",
    "    emails = []\n",
    "    \n",
    "    for msg in messages:\n",
    "        msg_id = msg[\"id\"]\n",
    "        message = service.users().messages().get(userId=\"me\", id=msg_id, format=\"metadata\").execute()\n",
    "        \n",
    "        payload = message.get(\"payload\", {}).get(\"headers\", [])\n",
    "        subject = next((header[\"value\"] for header in payload if header[\"name\"] == \"Subject\"), \"No Subject\")\n",
    "        timestamp = message.get(\"internalDate\", \"0\")  # Internal timestamp in milliseconds\n",
    "        body = message.get(\"snippet\", \"\")\n",
    "        \n",
    "        emails.append({\n",
    "            \"id\": msg_id,\n",
    "            \"subject\": subject,\n",
    "            \"body\": body,\n",
    "            \"timestamp\": int(timestamp)  # Convert to integer for sorting\n",
    "        })\n",
    "\n",
    "    # ‚úÖ Sort emails chronologically (oldest first)\n",
    "    emails.sort(key=lambda x: x[\"timestamp\"])\n",
    "    \n",
    "    return emails\n",
    "\n",
    "# üîπ Enhanced Email Classification using NLP\n",
    "def classify_email(email):\n",
    "    subject, body = email[\"subject\"], email[\"body\"]\n",
    "    email_text = f\"{subject} {body}\".lower()\n",
    "\n",
    "    if any(keyword in email_text for keyword in [\"job\", \"application\", \"intern\", \"hiring\", \"recruiter\"]):\n",
    "        return \"recruitment\"\n",
    "    elif any(keyword in email_text for keyword in [\"assignment\", \"course\", \"event\", \"exam\", \"lecture\"]):\n",
    "        return \"school\"\n",
    "    elif any(keyword in email_text for keyword in [\"newsletter\", \"promotion\", \"discount\", \"offer\"]):\n",
    "        return \"promotions\"\n",
    "    elif any(keyword in email_text for keyword in [\"verification\", \"subscription\", \"notification\", \"alert\"]):\n",
    "        return \"general\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# üîπ Store Emails in a Structured Way\n",
    "def store_emails(emails):\n",
    "    base_path = \"email-knowledge-base\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    for email in emails:\n",
    "        category = classify_email(email)\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        os.makedirs(category_path, exist_ok=True)\n",
    "        \n",
    "        file_path = os.path.join(category_path, f\"{email['id']}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Timestamp: {email['timestamp']}\\n\")\n",
    "            f.write(f\"Subject: {email['subject']}\\n\\n{email['body']}\")\n",
    "\n",
    "# üîπ Load Emails into Vector Database with Chronological Metadata\n",
    "def load_into_vector_db():\n",
    "    folders = glob.glob(\"email-knowledge-base/*\")\n",
    "\n",
    "    documents = []\n",
    "    for folder in folders:\n",
    "        doc_type = os.path.basename(folder)\n",
    "        loader = DirectoryLoader(folder, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "        folder_docs = loader.load()\n",
    "        for doc in folder_docs:\n",
    "            doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.extend(folder_docs)\n",
    "\n",
    "    # ‚úÖ Extract timestamp safely, default to 0 if missing\n",
    "    def extract_timestamp(doc):\n",
    "        lines = doc.page_content.split(\"\\n\")\n",
    "        if lines and lines[0].startswith(\"Timestamp: \"):\n",
    "            try:\n",
    "                return int(lines[0].replace(\"Timestamp: \", \"\").strip())  # Convert timestamp\n",
    "            except ValueError:\n",
    "                return 0  # If conversion fails, set to 0 (oldest)\n",
    "        return 0  # Default to 0 if no timestamp is found\n",
    "\n",
    "    # ‚úÖ Sort documents based on extracted timestamps\n",
    "    documents.sort(key=extract_timestamp)\n",
    "\n",
    "    # ‚úÖ Adaptive chunking strategy (split by sentence)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separators=[\"\\n\", \" \", \"\"])\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Total number of chunks: {len(chunks)}\")\n",
    "    print(f\"Email categories found: {set(doc.metadata['doc_type'] for doc in documents)}\")\n",
    "\n",
    "    # ‚úÖ Print the first and last processed email for verification\n",
    "    if documents:\n",
    "        first_timestamp = extract_timestamp(documents[0])\n",
    "        last_timestamp = extract_timestamp(documents[-1])\n",
    "        print(f\"üì© First processed email timestamp: {first_timestamp}\")\n",
    "        print(f\"üì© Last processed email timestamp: {last_timestamp}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# üîπ Query Function with Improved Retrieval\n",
    "def query_email_rag(query, index, chunks):\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "\n",
    "    # ‚úÖ Retrieve the top 3 most relevant chunks\n",
    "    distances, indices = index.search(np.array([query_embedding], dtype=np.float32), k=3)\n",
    "    \n",
    "    relevant_chunks = [chunks[i].page_content for i in indices[0]]\n",
    "    \n",
    "    print(\"üîç Top Matching Chunks:\")\n",
    "    for chunk in relevant_chunks:\n",
    "        print(f\"üìú {chunk[:300]}...\\n\")\n",
    "\n",
    "    return relevant_chunks\n",
    "\n",
    "# üîπ Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    service = load_gmail_service()\n",
    "    emails = fetch_emails(service, max_results=300)\n",
    "    store_emails(emails)\n",
    "    chunks = load_into_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c2f25ca-ad97-4bef-a83e-4fbc428a0002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of emails processed: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of emails processed: {len(emails)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf452d88-edbe-4b71-a8db-74bccf57dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 4504 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb6429b1-8446-4917-90b2-0ad9c42a193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "31078184-755d-4890-b515-fb3490ba93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "584fa740-5a15-4b7a-8a2e-dfe77a0bd07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681823b-68de-4fea-b132-836d08209687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
